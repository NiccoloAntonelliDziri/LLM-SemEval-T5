{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75e4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from pydantic import BaseModel\n",
    "import subprocess\n",
    "import gc\n",
    "import pandas as pd\n",
    "import json\n",
    "from icecream import ic\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ones installed on my pc\n",
    "model_names = [\n",
    "    \"llama3.1\",\n",
    "    \"olmo-3:7b-instruct\",\n",
    "    \"olmo-3\",        # Cannot disable the thinking here but it's essentially the preivous one\n",
    "    \"granite3.3\",\n",
    "    \"ministral-3\",\n",
    "    \"qwen3\",\n",
    "    \"qwen2.5-coder\",\n",
    "    \"deepseek-r1\",   # here with no thinking\n",
    "    \"deepseek-r1\",\n",
    "    \"gemma3\",\n",
    "    \"phi4-mini\",\n",
    "]\n",
    "\n",
    "model_thinking = [\n",
    "    False,\n",
    "    False,\n",
    "    True,\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "    True,\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc50137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"\"\"\n",
    "    You are an expert NLU annotator. Your job is to rate how plausible a candidate meaning (sense)\n",
    "    is for the HOMONYM used in the target sentence within the short story.\n",
    "\n",
    "    Return ONLY a single JSON object with one key: \"score\" and an integer value 1, 2, 3, 4 or 5.\n",
    "    Integer mapping:\n",
    "      1 = Definitely not\n",
    "      2 = Probably not\n",
    "      3 = Ambiguous / Unsure\n",
    "      4 = Probably yes\n",
    "      5 = Definitely yes\n",
    "\n",
    "    The response must be a JSON object and nothing else, for example: {{\"score\": 4}}\n",
    "\n",
    "    [EXAMPLES]\n",
    "    {few_shot_examples}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "USER_PROMPT = (\n",
    "    \"\"\"\n",
    "    Now, label this new instance:\n",
    "\n",
    "    [STORY]\n",
    "    {full_story_text}\n",
    "\n",
    "    [HOMONYM]\n",
    "    {homonym}\n",
    "\n",
    "    [CANDIDATE SENSE]\n",
    "    {sense_text}\n",
    "\n",
    "    [TASK]\n",
    "    Based on the STORY above, decide how plausible it is that the HOMONYM is used with the\n",
    "    CANDIDATE SENSE in the target sentence.\n",
    "\n",
    "    Return ONLY a single JSON object with one key \"score\" and an integer value (1-5)\n",
    "    as described by the system message. Example output: {{\"score\": 3}}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def build_few_shot_examples(df):\n",
    "    \"\"\"Build a five-shot examples string (one example per score 1-5) from `df`.\n",
    "\n",
    "    Uses rounded average to determine the representative score for an item.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    used_ids = set()\n",
    "\n",
    "    for score in range(1, 6):\n",
    "        found = False\n",
    "        for idx, row in df.iterrows():\n",
    "            if str(idx) in used_ids:\n",
    "                continue\n",
    "            try:\n",
    "                avg = float(row.get(\"average\", 0))\n",
    "            except Exception:\n",
    "                avg = 0.0\n",
    "            if round(avg) == score:\n",
    "                full = create_full_story_text(row)\n",
    "                sense = f\"{row.get('judged_meaning', '')} as in \\\"{row.get('example_sentence','')}\\\"\"\n",
    "                ex = (\n",
    "                    \"[STORY]\\n\"\n",
    "                    f\"{full}\\n\\n\"\n",
    "                    \"[HOMONYM]\\n\"\n",
    "                    f\"{row.get('homonym', '')}\\n\\n\"\n",
    "                    \"[CANDIDATE SENSE]\\n\"\n",
    "                    f\"{sense}\\n\\n\"\n",
    "                    \"[EXAMPLE OUTPUT]\\n\"\n",
    "                    f'{{\"score\": {score}}}'\n",
    "                )\n",
    "                examples.append(ex)\n",
    "                used_ids.add(str(idx))\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            # fallback: include a short template example if no example found for this score\n",
    "            examples.append(f\"[EXAMPLE OUTPUT]\\n{{\\\"score\\\": {score}}}\")\n",
    "\n",
    "    return \"\\n\\n\".join(examples)\n",
    "\n",
    "def create_full_story_text(item):\n",
    "    \"\"\"Compose the story text used as context for rating.\n",
    "\n",
    "    Uses `precontext`, `sentence`, and `ending` fields when available and joins them into a single string.\n",
    "    \"\"\"\n",
    "    fullstory = f\"{item.get('precontext', '')} {item.get('sentence', '')} {item.get('ending', '')}\"\n",
    "    return fullstory.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8189a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_JSON_FILE = \"../data/train.json\"\n",
    "DEV_JSON_FILE = \"../data/dev.json\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the json containing the dataset and return a pandas dataframe.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Transpose because the json is {id: {features...}, ...}\n",
    "    df = pd.DataFrame(data).T\n",
    "    # Ensure 'average' is float\n",
    "    df['average'] = df['average'].astype(float)\n",
    "    # Ensure 'choices' is list (for scoring later)\n",
    "    return df\n",
    "\n",
    "df_train = load_data(TRAIN_JSON_FILE)\n",
    "df_dev = load_data(DEV_JSON_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12423bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mfew_shot_examples\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'''\u001b[39m\u001b[38;5;36m[STORY]\u001b[39m\n",
      "\u001b[38;5;36m                        The troops were stationed at a remote camp for training. To pass the time, they often engaged in recreational sports. One afternoon, they decided to play a game on the field next to their tents. The soldiers were playing a bat and ball game, which involved them running towards the base. It didn\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mt take long for the winning team to get a home run.\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [HOMONYM]\u001b[39m\n",
      "\u001b[38;5;36m                        base\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [CANDIDATE SENSE]\u001b[39m\n",
      "\u001b[38;5;36m                        installation from which a military force initiates operations as in \u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mThe troops strategized at their forward base.\u001b[39m\u001b[38;5;36m\"\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [EXAMPLE OUTPUT]\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\u001b[38;5;36m{\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mscore\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36m: 1}\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [STORY]\u001b[39m\n",
      "\u001b[38;5;36m                        The old machine hummed in the corner of the workshop. Clara examined its dusty dials with a furrowed brow. She wondered if it could be brought back to life. The potential couldn\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mt be measured. The machine could make such wonderful clothing if it were in working order.\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [HOMONYM]\u001b[39m\n",
      "\u001b[38;5;36m                        potential\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [CANDIDATE SENSE]\u001b[39m\n",
      "\u001b[38;5;36m                        the difference in electrical charge between two points in a circuit expressed in volts as in \u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mThe circuit has a high potential difference.\u001b[39m\u001b[38;5;36m\"\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [EXAMPLE OUTPUT]\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\u001b[38;5;36m{\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mscore\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36m: 2}\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [STORY]\u001b[39m\n",
      "\u001b[38;5;36m                        The old machine hummed in the corner of the workshop. Clara examined its dusty dials with a furrowed brow. She wondered if it could be brought back to life. The potential couldn\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mt be measured. She collected a battery reader and looked on earnestly, willing some life back into the old machine.\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [HOMONYM]\u001b[39m\n",
      "\u001b[38;5;36m                        potential\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [CANDIDATE SENSE]\u001b[39m\n",
      "\u001b[38;5;36m                        the difference in electrical charge between two points in a circuit expressed in volts as in \u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mThe circuit has a high potential difference.\u001b[39m\u001b[38;5;36m\"\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [EXAMPLE OUTPUT]\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\u001b[38;5;36m{\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mscore\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36m: 3}\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [STORY]\u001b[39m\n",
      "\u001b[38;5;36m                        The old machine hummed in the corner of the workshop. Clara examined its dusty dials with a furrowed brow. She wondered if it could be brought back to life. The potential couldn\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mt be measured. She collected a battery reader and looked on earnestly, willing some life back into the old machine.\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [HOMONYM]\u001b[39m\n",
      "\u001b[38;5;36m                        potential\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [CANDIDATE SENSE]\u001b[39m\n",
      "\u001b[38;5;36m                        the inherent capacity for coming into being as in \u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mThe project has great potential for success.\u001b[39m\u001b[38;5;36m\"\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [EXAMPLE OUTPUT]\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\u001b[38;5;36m{\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mscore\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36m: 4}\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [STORY]\u001b[39m\n",
      "\u001b[38;5;36m                        The troops were stationed at a remote camp for training. To pass the time, they often engaged in recreational sports. One afternoon, they decided to play a game on the field next to their tents. The soldiers were playing a bat and ball game, which involved them running towards the base. It didn\u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mt take long for the winning team to get a home run.\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [HOMONYM]\u001b[39m\n",
      "\u001b[38;5;36m                        base\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [CANDIDATE SENSE]\u001b[39m\n",
      "\u001b[38;5;36m                        a place that the runner must touch before scoring as in \u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mHe slid into third base safely.\u001b[39m\u001b[38;5;36m\"\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\n",
      "\u001b[38;5;36m                        [EXAMPLE OUTPUT]\u001b[39m\n",
      "\u001b[38;5;36m                        \u001b[39m\u001b[38;5;36m{\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mscore\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36m: 5}\u001b[39m\u001b[38;5;36m'''\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Build and expose the few-shot examples string used by prompts\n",
    "few_shot_examples = build_few_shot_examples(df_train)\n",
    "ic(few_shot_examples)\n",
    "\n",
    "def create_message(item):\n",
    "    sense = f\"{item.get('judged_meaning', '')} as in \\\"{item.get('example_sentence', '')}\\\"\".strip()\n",
    "    homonym = item.get(\"homonym\", \"\")\n",
    "    full_story_text = create_full_story_text(item)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT.format(\n",
    "            few_shot_examples = few_shot_examples\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT.format(\n",
    "            full_story_text=full_story_text,\n",
    "            homonym=homonym,\n",
    "            sense_text=sense\n",
    "        )},\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30379453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score(BaseModel):\n",
    "    score: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa993d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m messages = create_message(item)\n\u001b[32m      6\u001b[39m model_number = \u001b[32m6\u001b[39m  \u001b[38;5;66;03m# change to try different models\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response: ChatResponse = chat(model=\u001b[43mmodel_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_number\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m      9\u001b[39m                               messages=messages,\n\u001b[32m     10\u001b[39m                               think=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     11\u001b[39m                               \u001b[38;5;28mformat\u001b[39m=Score.model_json_schema(),\n\u001b[32m     12\u001b[39m                               options={\n\u001b[32m     13\u001b[39m                                   \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m\n\u001b[32m     14\u001b[39m                               }\n\u001b[32m     15\u001b[39m                               )\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ic(messages)\u001b[39;00m\n\u001b[32m     18\u001b[39m ic(response.model)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Random element in the dataset\n",
    "item = df_train.sample(1).iloc[0].to_dict()\n",
    "\n",
    "messages = create_message(item)\n",
    "\n",
    "model_number = 6  # change to try different models\n",
    "\n",
    "response: ChatResponse = chat(model=model_names[model_number],\n",
    "                              messages=messages,\n",
    "                              think=False,\n",
    "                              format=Score.model_json_schema(),\n",
    "                              options={\n",
    "                                  \"temperature\": 0\n",
    "                              }\n",
    "                              )\n",
    "\n",
    "# ic(messages)\n",
    "ic(response.model)\n",
    "ic(response.total_duration * 10e-9)  # convert from ns to s\n",
    "ic(response.message.role)\n",
    "ic(response.message.content)\n",
    "ic(response.message.thinking)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c33ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_predictions(model_name, df, max_examples=None, think=False):\n",
    "    preds = []\n",
    "    failed_ids = []\n",
    "    ids = list(df.index.astype(str))\n",
    "    if max_examples is not None:\n",
    "        ids = ids[:max_examples]\n",
    "\n",
    "    total_start = time.perf_counter()\n",
    "    per_item_times = []\n",
    "\n",
    "    for idx in tqdm(ids):\n",
    "        item = df.loc[idx].to_dict()\n",
    "        messages = create_message(item)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        try:\n",
    "            response: ChatResponse = chat(model=model_name,\n",
    "                                          messages=messages,\n",
    "                                          think=think,          # No thinking because otherwise takes ~ 6hours for the whole dev dataset per thinking model\n",
    "                                          format=Score.model_json_schema(),\n",
    "                                          options={\n",
    "                                              \"temperature\": 0\n",
    "                                          }\n",
    "                                          )\n",
    "            elapsed = time.perf_counter() - start\n",
    "\n",
    "            per_item_times.append((idx, elapsed))\n",
    "\n",
    "            content = response.message.content\n",
    "            try:\n",
    "                s = Score.model_validate_json(content)\n",
    "                pred = int(s.score)\n",
    "                if pred < 1 or pred > 5:\n",
    "                    raise ValueError(\"score out of range\")\n",
    "            except Exception:\n",
    "                # Keep id of failed element so it can be removed from evaluation\n",
    "                print(\"Invalid JSON or missing/invalid score for item id:\", idx, \"content:\", content)\n",
    "                pred = None\n",
    "                failed_ids.append(str(idx))\n",
    "\n",
    "        except Exception as e:\n",
    "            elapsed = time.perf_counter() - start\n",
    "            print(f\"Error calling model {model_name} for id {idx}: {e}\")\n",
    "            pred = None\n",
    "            failed_ids.append(str(idx))\n",
    "            per_item_times.append((idx, elapsed))\n",
    "\n",
    "        preds.append({\"id\": str(idx), \"prediction\": pred, \"time\": elapsed})\n",
    "\n",
    "    total_elapsed = time.perf_counter() - total_start\n",
    "    # attach summary timings as metadata and failed ids\n",
    "    return {\n",
    "        \"predictions\": preds,\n",
    "        \"failed_ids\": failed_ids,\n",
    "        \"total_time\": total_elapsed,\n",
    "        \"per_item_times\": per_item_times,\n",
    "        \"avg_time\": sum(t for _, t in per_item_times) / len(per_item_times) if per_item_times else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ae9cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running model: smollm ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [02:10<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/five-shot/smollm/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/five-shot/smollm/ref.jsonl\n",
      "Timing saved to ../llm-ollama/five-shot/smollm/timing.txt\n",
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/five-shot/smollm/predictions.jsonl on ../llm-ollama/five-shot/smollm/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.050198506980056797\n",
      "Spearman p-Value: 0.22420394764592197\n",
      "----------\n",
      "Accuracy: 0.5238095238095238 (308/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "for model_name, think in zip(model_names, model_thinking):\n",
    "    if think:\n",
    "        MAX_EXAMPLES = 100  # set to an int to limit samples per model\n",
    "    else:\n",
    "        MAX_EXAMPLES = None  # set to None for not thinking models\n",
    "\n",
    "    print(f\"\\n=== Running model: {model_name} ===\")\n",
    "\n",
    "    # If deepseek check if thinking to have different directoies\n",
    "    if think:\n",
    "        OUT_DIR = f\"../llm-ollama/five-shot/{model_name.replace(':', '-')}-think\"\n",
    "        os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    else:\n",
    "        OUT_DIR = f\"../llm-ollama/five-shot/{model_name.replace(':', '-')}\"\n",
    "        os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # get predictions (may take a while if MAX_EXAMPLES is None)\n",
    "    # res = get_dev_predictions(model_name, df_dev, max_examples=MAX_EXAMPLES)\n",
    "    res = get_dev_predictions(model_name, df_dev, max_examples=MAX_EXAMPLES, think=think)\n",
    "\n",
    "    preds = res[\"predictions\"]\n",
    "\n",
    "    pred_file = os.path.join(OUT_DIR, \"predictions.jsonl\")\n",
    "    with open(pred_file, \"w\") as f:\n",
    "        for p in preds:\n",
    "            f.write(json.dumps({\"id\": p[\"id\"], \"prediction\": p[\"prediction\"]}) + \"\\n\")\n",
    "\n",
    "    # Save failed ids so they can be excluded from scoring\n",
    "    failed_file = os.path.join(OUT_DIR, \"failed_ids.jsonl\")\n",
    "    with open(failed_file, \"w\") as f:\n",
    "        for fid in res.get(\"failed_ids\", []):\n",
    "            f.write(json.dumps({\"id\": fid}) + \"\\n\")\n",
    "\n",
    "    # Save timing info and per-item times\n",
    "    timing_file = os.path.join(OUT_DIR, \"timing.txt\")\n",
    "    with open(timing_file, \"w\") as f:\n",
    "        f.write(f\"total_time_sec: {res['total_time']:.4f}\\n\")\n",
    "        f.write(f\"avg_time_sec: {res['avg_time']:.4f}\\n\")\n",
    "        f.write(\"per_item_times_sec:\\n\")\n",
    "        for idx, t in res[\"per_item_times\"]:\n",
    "            f.write(f\"{idx}: {t:.4f}\\n\")\n",
    "\n",
    "    # Create ref.jsonl from df_dev (respect MAX_EXAMPLES) inside the model folder\n",
    "    # Exclude any ids that failed JSON parsing so they won't be evaluated\n",
    "    failed_set = set(res.get(\"failed_ids\", []))\n",
    "    ref_file = os.path.join(OUT_DIR, \"ref.jsonl\")\n",
    "    with open(ref_file, \"w\") as f:\n",
    "        for idx, row in df_dev.iterrows():\n",
    "            if MAX_EXAMPLES is not None and int(idx) >= MAX_EXAMPLES:\n",
    "                break\n",
    "            if str(idx) in failed_set:\n",
    "                # skip items that produced invalid JSON for this model\n",
    "                continue\n",
    "            f.write(json.dumps({\"id\": str(idx), \"label\": row[\"choices\"]}) + \"\\n\")\n",
    "\n",
    "    print(f\"Predictions saved to {pred_file}\")\n",
    "    print(f\"Gold data saved to {ref_file}\")\n",
    "    print(f\"Timing saved to {timing_file}\")\n",
    "\n",
    "    # If there is failed attemp rewrite all of the ids so that they are consecutive. This is needed for the scoring script\n",
    "    if len(res.get('failed_ids', [])) > 0:\n",
    "        # Rewrite pred_file with consecutive ids\n",
    "        new_pred_file = os.path.join(OUT_DIR, \"predictions_consecutive_ids.jsonl\")\n",
    "        with open(pred_file, \"r\") as fin, open(new_pred_file, \"w\") as fout:\n",
    "            for new_id, line in enumerate(fin):\n",
    "                obj = json.loads(line)\n",
    "                obj[\"id\"] = str(new_id)\n",
    "                fout.write(json.dumps(obj) + \"\\n\")\n",
    "        pred_file = new_pred_file\n",
    "\n",
    "        # Rewrite ref_file with consecutive ids\n",
    "        new_ref_file = os.path.join(OUT_DIR, \"ref_consecutive_ids.jsonl\")\n",
    "        with open(ref_file, \"r\") as fin, open(new_ref_file, \"w\") as fout:\n",
    "            for new_id, line in enumerate(fin):\n",
    "                obj = json.loads(line)\n",
    "                obj[\"id\"] = str(new_id)\n",
    "                fout.write(json.dumps(obj) + \"\\n\")\n",
    "        ref_file = new_ref_file\n",
    "        \n",
    "    # Run scoring script for this model outputs\n",
    "    res = subprocess.run([\"python\", \"../score/scoring.py\", ref_file, pred_file, os.path.join(OUT_DIR, \"score.json\")], capture_output=True, text=True)\n",
    "    print(res.stdout)\n",
    "    if res.stderr:\n",
    "        print(\"Scoring STDERR:\")\n",
    "        print(res.stderr)\n",
    "\n",
    "    # \n",
    "    subprocess.run([\"ollama\", \"stop\", model_name], check=False)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32541ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
