{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75e4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from pydantic import BaseModel\n",
    "import subprocess\n",
    "import gc\n",
    "import pandas as pd\n",
    "import json\n",
    "from icecream import ic\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ones installed on my pc\n",
    "model_names = [\n",
    "    \"llama3.1\",\n",
    "    \"olmo-3:7b-instruct\",\n",
    "    \"olmo-3\",        # Cannot disable the thinking here but it's essentially the preivous one\n",
    "    \"granite3.3\",\n",
    "    \"ministral-3\",\n",
    "    \"qwen3\",\n",
    "    \"qwen2.5-coder\",\n",
    "    \"deepseek-r1\",   # here with no thinking\n",
    "    \"deepseek-r1\",\n",
    "    \"gemma3\",\n",
    "    \"phi4-mini\",\n",
    "]\n",
    "\n",
    "model_thinking = [\n",
    "    False,\n",
    "    False,\n",
    "    True,\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "    True,\n",
    "    False,\n",
    "    False,\n",
    "    False\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c7ab610",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"\"\"\n",
    "    You are an expert NLU annotator. Your job is to rate how plausible a candidate meaning (sense)\n",
    "    is for the HOMONYM used in the target sentence within the short story.\n",
    "\n",
    "    Return ONLY a single JSON object with one key: \"score\" and an integer value 1, 2, 3, 4 or 5.\n",
    "    Integer mapping:\n",
    "      1 = Definitely not\n",
    "      2 = Probably not\n",
    "      3 = Ambiguous / Unsure\n",
    "      4 = Probably yes\n",
    "      5 = Definitely yes\n",
    "\n",
    "    The response must be a JSON object and nothing else, for example: {{\"score\": 4}}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "USER_PROMPT = (\n",
    "    \"\"\"\n",
    "    [STORY]\n",
    "    {full_story_text}\n",
    "\n",
    "    [HOMONYM]\n",
    "    {homonym}\n",
    "\n",
    "    [CANDIDATE SENSE]\n",
    "    {sense_text}\n",
    "\n",
    "    [TASK]\n",
    "    Based on the STORY above, decide how plausible it is that the HOMONYM is used with the\n",
    "    CANDIDATE SENSE in the target sentence.\n",
    "\n",
    "    Return ONLY a single JSON object with one key \"score\" and an integer value (1-5)\n",
    "    as described by the system message. Example output: {{\"score\": 3}}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc50137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_story_text(item):\n",
    "    \"\"\"Compose the story text used as context for rating.\n",
    "\n",
    "    Uses `precontext`, `sentence`, and `ending` fields when available and joins them into a single string.\n",
    "    \"\"\"\n",
    "    fullstory = f\"{item.get('precontext', '')} {item.get('sentence', '')} {item.get('ending', '')}\"\n",
    "    return fullstory.strip()\n",
    "\n",
    "\n",
    "def create_message(item):\n",
    "    sense = f\"{item.get('judged_meaning', '')} as in \\\"{item.get('example_sentence', '')}\\\"\".strip()\n",
    "    homonym = item.get(\"homonym\", \"\")\n",
    "    full_story_text = create_full_story_text(item)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT.format(\n",
    "            full_story_text=full_story_text,\n",
    "            homonym=homonym,\n",
    "            sense_text=sense,\n",
    "        )},\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8189a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_JSON_FILE = \"../data/train.json\"\n",
    "DEV_JSON_FILE = \"../data/dev.json\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the json containing the dataset and return a pandas dataframe.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Transpose because the json is {id: {features...}, ...}\n",
    "    df = pd.DataFrame(data).T\n",
    "    # Ensure 'average' is float\n",
    "    df['average'] = df['average'].astype(float)\n",
    "    # Ensure 'choices' is list (for scoring later)\n",
    "    return df\n",
    "\n",
    "df_train = load_data(TRAIN_JSON_FILE)\n",
    "df_dev = load_data(DEV_JSON_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30379453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score(BaseModel):\n",
    "    score: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afa993d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mmodel\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mqwen2.5-coder\u001b[39m\u001b[38;5;36m'\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mtotal_duration\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;245m*\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m10e-9\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m88.62625398\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mmessage\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mrole\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36massistant\u001b[39m\u001b[38;5;36m'\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mmessage\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mcontent\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36m{\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mscore\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36m: 4}\u001b[39m\u001b[38;5;36m'\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mmessage\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mthinking\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;100mNone\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Random element in the dataset\n",
    "item = df_train.sample(1).iloc[0].to_dict()\n",
    "\n",
    "messages = create_message(item)\n",
    "\n",
    "model_number = 6  # change to try different models\n",
    "\n",
    "response: ChatResponse = chat(model=\"qwen2.5-coder\",\n",
    "                              messages=messages,\n",
    "                              think=False,\n",
    "                              format=Score.model_json_schema(),\n",
    "                              options={\n",
    "                                  \"temperature\": 0\n",
    "                              }\n",
    "                              )\n",
    "\n",
    "# ic(messages)\n",
    "ic(response.model)\n",
    "ic(response.total_duration * 10e-9)  # convert from ns to s\n",
    "ic(response.message.role)\n",
    "ic(response.message.content)\n",
    "ic(response.message.thinking)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22c33ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_predictions(model_name, df, max_examples=None, think=False):\n",
    "    preds = []\n",
    "    failed_ids = []\n",
    "    ids = list(df.index.astype(str))\n",
    "    if max_examples is not None:\n",
    "        ids = ids[:max_examples]\n",
    "\n",
    "    total_start = time.perf_counter()\n",
    "    per_item_times = []\n",
    "\n",
    "    for idx in tqdm(ids):\n",
    "        item = df.loc[idx].to_dict()\n",
    "        messages = create_message(item)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        try:\n",
    "            response: ChatResponse = chat(model=model_name,\n",
    "                                          messages=messages,\n",
    "                                          think=think,\n",
    "                                          format=Score.model_json_schema(),\n",
    "                                          options={\n",
    "                                              \"temperature\": 0\n",
    "                                          }\n",
    "                                          )\n",
    "            elapsed = time.perf_counter() - start\n",
    "\n",
    "            per_item_times.append((idx, elapsed))\n",
    "\n",
    "            content = response.message.content\n",
    "            try:\n",
    "                s = Score.model_validate_json(content)\n",
    "                pred = int(s.score)\n",
    "                if pred < 1 or pred > 5:\n",
    "                    raise ValueError(\"score out of range\")\n",
    "            except Exception:\n",
    "                # Keep id of failed element so it can be removed from evaluation\n",
    "                print(\"Invalid JSON or missing/invalid score for item id:\", idx, \"content:\", content)\n",
    "                pred = None\n",
    "                failed_ids.append(str(idx))\n",
    "\n",
    "        except Exception as e:\n",
    "            elapsed = time.perf_counter() - start\n",
    "            print(f\"Error calling model {model_name} for id {idx}: {e}\")\n",
    "            pred = None\n",
    "            failed_ids.append(str(idx))\n",
    "            per_item_times.append((idx, elapsed))\n",
    "\n",
    "        preds.append({\"id\": str(idx), \"prediction\": pred, \"time\": elapsed})\n",
    "\n",
    "    total_elapsed = time.perf_counter() - total_start\n",
    "    # attach summary timings as metadata and failed ids\n",
    "    return {\n",
    "        \"predictions\": preds,\n",
    "        \"failed_ids\": failed_ids,\n",
    "        \"total_time\": total_elapsed,\n",
    "        \"per_item_times\": per_item_times,\n",
    "        \"avg_time\": sum(t for _, t in per_item_times) / len(per_item_times) if per_item_times else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31ae9cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running model: smollm ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [02:02<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/zero-shot/smollm/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/zero-shot/smollm/ref.jsonl\n",
      "Timing saved to ../llm-ollama/zero-shot/smollm/timing.txt\n",
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/zero-shot/smollm/predictions.jsonl on ../llm-ollama/zero-shot/smollm/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: nan\n",
      "Spearman p-Value: nan\n",
      "----------\n",
      "Accuracy: 0.5272108843537415 (310/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "Scoring STDERR:\n",
      "/home/niccolo/Torino/LLM-SamEval-T5/scripts/../score/scoring.py:62: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, value = spearmanr(pred_list, gold_list)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "for model_name, think in zip(model_names, model_thinking):\n",
    "    if think:\n",
    "        MAX_EXAMPLES = 100  # set to an int to limit samples per model\n",
    "    else:\n",
    "        MAX_EXAMPLES = None  # set to None for not thinking models\n",
    "\n",
    "    print(f\"\\n=== Running model: {model_name} ===\")\n",
    "\n",
    "    # If deepseek check if thinking to have different directoies\n",
    "    if think:\n",
    "        OUT_DIR = f\"../llm-ollama/zero-shot/{model_name.replace(':', '-')}-think\"\n",
    "        os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    else:\n",
    "        OUT_DIR = f\"../llm-ollama/zero-shot/{model_name.replace(':', '-')}\"\n",
    "        os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # get predictions (may take a while if MAX_EXAMPLES is None)\n",
    "    # res = get_dev_predictions(model_name, df_dev, max_examples=MAX_EXAMPLES)\n",
    "    res = get_dev_predictions(model_name, df_dev, max_examples=MAX_EXAMPLES, think=think)\n",
    "\n",
    "    preds = res[\"predictions\"]\n",
    "\n",
    "    pred_file = os.path.join(OUT_DIR, \"predictions.jsonl\")\n",
    "    # Only write successful predictions (skip None) so pred/ref sizes are aligned\n",
    "    with open(pred_file, \"w\") as f:\n",
    "        for p in preds:\n",
    "            if p[\"prediction\"] is None:\n",
    "                # skip failed predictions (they are recorded in failed_ids)\n",
    "                continue\n",
    "            f.write(json.dumps({\"id\": p[\"id\"], \"prediction\": p[\"prediction\"]}) + \"\\n\")\n",
    "\n",
    "    # Save failed ids so they can be excluded from scoring\n",
    "    failed_file = os.path.join(OUT_DIR, \"failed_ids.jsonl\")\n",
    "    with open(failed_file, \"w\") as f:\n",
    "        for fid in res.get(\"failed_ids\", []):\n",
    "            f.write(json.dumps({\"id\": fid}) + \"\\n\")\n",
    "\n",
    "    # Save timing info and per-item times\n",
    "    timing_file = os.path.join(OUT_DIR, \"timing.txt\")\n",
    "    with open(timing_file, \"w\") as f:\n",
    "        f.write(f\"total_time_sec: {res['total_time']:.4f}\\n\")\n",
    "        f.write(f\"avg_time_sec: {res['avg_time']:.4f}\\n\")\n",
    "        f.write(\"per_item_times_sec:\\n\")\n",
    "        for idx, t in res[\"per_item_times\"]:\n",
    "            f.write(f\"{idx}: {t:.4f}\\n\")\n",
    "\n",
    "    # Create ref.jsonl from df_dev (respect MAX_EXAMPLES) inside the model folder\n",
    "    # Exclude any ids that failed JSON parsing so they won't be evaluated\n",
    "    failed_set = set(res.get(\"failed_ids\", []))\n",
    "    ref_file = os.path.join(OUT_DIR, \"ref.jsonl\")\n",
    "    with open(ref_file, \"w\") as f:\n",
    "        for idx, row in df_dev.iterrows():\n",
    "            if MAX_EXAMPLES is not None and int(idx) >= MAX_EXAMPLES:\n",
    "                break\n",
    "            if str(idx) in failed_set:\n",
    "                # skip items that produced invalid JSON for this model\n",
    "                continue\n",
    "            f.write(json.dumps({\"id\": str(idx), \"label\": row[\"choices\"]}) + \"\\n\")\n",
    "\n",
    "    print(f\"Predictions saved to {pred_file}\")\n",
    "    print(f\"Gold data saved to {ref_file}\")\n",
    "    print(f\"Timing saved to {timing_file}\")\n",
    "\n",
    "    # Sanity check: warn if counts differ\n",
    "    n_preds = sum(1 for _ in open(pred_file, \"r\"))\n",
    "    n_refs = sum(1 for _ in open(ref_file, \"r\"))\n",
    "    if n_preds != n_refs:\n",
    "        print(f\"Warning: #preds ({n_preds}) != #refs ({n_refs}). failed_ids_len={len(res.get('failed_ids', []))}\")\n",
    "\n",
    "    # If there is failed attemp rewrite all of the ids so that they are consecutive. This is needed for the scoring script\n",
    "    if len(res.get('failed_ids', [])) > 0:\n",
    "        # Rewrite pred_file with consecutive ids\n",
    "        new_pred_file = os.path.join(OUT_DIR, \"predictions_consecutive_ids.jsonl\")\n",
    "        with open(pred_file, \"r\") as fin, open(new_pred_file, \"w\") as fout:\n",
    "            for new_id, line in enumerate(fin):\n",
    "                obj = json.loads(line)\n",
    "                obj[\"id\"] = str(new_id)\n",
    "                fout.write(json.dumps(obj) + \"\\n\")\n",
    "        pred_file = new_pred_file\n",
    "\n",
    "        # Rewrite ref_file with consecutive ids\n",
    "        new_ref_file = os.path.join(OUT_DIR, \"ref_consecutive_ids.jsonl\")\n",
    "        with open(ref_file, \"r\") as fin, open(new_ref_file, \"w\") as fout:\n",
    "            for new_id, line in enumerate(fin):\n",
    "                obj = json.loads(line)\n",
    "                obj[\"id\"] = str(new_id)\n",
    "                fout.write(json.dumps(obj) + \"\\n\")\n",
    "        ref_file = new_ref_file\n",
    "\n",
    "    # Run scoring script for this model outputs\n",
    "    res = subprocess.run([\"python\", \"../score/scoring.py\", ref_file, pred_file, os.path.join(OUT_DIR, \"score.json\")], capture_output=True, text=True)\n",
    "    print(res.stdout)\n",
    "    if res.stderr:\n",
    "        print(\"Scoring STDERR:\")\n",
    "        print(res.stderr)\n",
    "\n",
    "    # \n",
    "    subprocess.run([\"ollama\", \"stop\", model_name], check=False)\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738daef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
