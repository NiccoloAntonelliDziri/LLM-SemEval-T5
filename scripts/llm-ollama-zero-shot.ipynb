{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75e4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from pydantic import BaseModel\n",
    "import subprocess\n",
    "import gc\n",
    "import pandas as pd\n",
    "import json\n",
    "from icecream import ic\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0bc662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ones installed on my pc\n",
    "model_names = [\n",
    "    \"llama3.1:latest\",\n",
    "    \"olmo-3:7b-instruct\",\n",
    "    # \"olmo-3:latest\",        # Cannot disable the thinking here but it's essentially the preivous one\n",
    "    \"granite3.3:latest\",\n",
    "    \"ministral-3:latest\",\n",
    "    \"qwen3:latest\",   \n",
    "    \"deepseek-r1:latest\",   # here with no thinking\n",
    "    \"gemma3:latest\",\n",
    "]\n",
    "\n",
    "# Just to know if they are thinking models\n",
    "support_thinking = [\n",
    "    False,\n",
    "    False,\n",
    "    # True,\n",
    "    False,\n",
    "    False,\n",
    "    True,\n",
    "    False,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc50137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"\"\"\n",
    "    You are an expert NLU annotator. Your job is to rate how plausible a candidate meaning (sense)\n",
    "    is for the HOMONYM used in the target sentence within the short story.\n",
    "\n",
    "    Return ONLY a single JSON object with one key: \"score\" and an integer value 1, 2, 3, 4 or 5.\n",
    "    Integer mapping:\n",
    "      1 = Definitely not\n",
    "      2 = Probably not\n",
    "      3 = Ambiguous / Unsure\n",
    "      4 = Probably yes\n",
    "      5 = Definitely yes\n",
    "\n",
    "    The response must be a JSON object and nothing else, for example: {{\"score\": 4}}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "USER_PROMPT = (\n",
    "    \"\"\"\n",
    "    [STORY]\n",
    "    {full_story_text}\n",
    "\n",
    "    [HOMONYM]\n",
    "    {homonym}\n",
    "\n",
    "    [CANDIDATE SENSE]\n",
    "    {sense_text}\n",
    "\n",
    "    [TASK]\n",
    "    Based on the STORY above, decide how plausible it is that the HOMONYM is used with the\n",
    "    CANDIDATE SENSE in the target sentence.\n",
    "\n",
    "    Return ONLY a single JSON object with one key \"score\" and an integer value (1-5)\n",
    "    as described by the system message. Example output: {{\"score\": 3}}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def create_full_story_text(item):\n",
    "    \"\"\"Compose the story text used as context for rating.\n",
    "\n",
    "    Uses `precontext`, `sentence`, and `ending` fields when available and joins them into a single string.\n",
    "    \"\"\"\n",
    "    fullstory = f\"{item.get('precontext', '')} {item.get('sentence', '')} {item.get('ending', '')}\"\n",
    "    return fullstory.strip()\n",
    "\n",
    "\n",
    "def create_message(item):\n",
    "    sense = f\"{item.get('judged_meaning', '')} as in \\\"{item.get('example_sentence', '')}\\\"\".strip()\n",
    "    homonym = item.get(\"homonym\", \"\")\n",
    "    full_story_text = create_full_story_text(item)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT.format(\n",
    "            full_story_text=full_story_text,\n",
    "            homonym=homonym,\n",
    "            sense_text=sense,\n",
    "        )},\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8189a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_JSON_FILE = \"../data/train.json\"\n",
    "DEV_JSON_FILE = \"../data/dev.json\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the json containing the dataset and return a pandas dataframe.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Transpose because the json is {id: {features...}, ...}\n",
    "    df = pd.DataFrame(data).T\n",
    "    # Ensure 'average' is float\n",
    "    df['average'] = df['average'].astype(float)\n",
    "    # Ensure 'choices' is list (for scoring later)\n",
    "    return df\n",
    "\n",
    "df_train = load_data(TRAIN_JSON_FILE)\n",
    "df_dev = load_data(DEV_JSON_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30379453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score(BaseModel):\n",
    "    score: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa993d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mmodel\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36mgemma3:latest\u001b[39m\u001b[38;5;36m'\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mtotal_duration\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;245m*\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m10e-9\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m47.26069995\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mmessage\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mrole\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'\u001b[39m\u001b[38;5;36massistant\u001b[39m\u001b[38;5;36m'\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mmessage\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mcontent\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m'''\u001b[39m\u001b[38;5;36m{\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36mscore\u001b[39m\u001b[38;5;36m\"\u001b[39m\u001b[38;5;36m: 2}\u001b[39m\n",
      "\u001b[38;5;36m                               \u001b[39m\u001b[38;5;36m'''\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mresponse\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mmessage\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mthinking\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;100mNone\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Random element in the dataset\n",
    "item = df_train.sample(1).iloc[0].to_dict()\n",
    "\n",
    "messages = create_message(item)\n",
    "\n",
    "model_number = 6  # change to try different models\n",
    "\n",
    "response: ChatResponse = chat(model=model_names[model_number],\n",
    "                              messages=messages,\n",
    "                              think=False,\n",
    "                              format=Score.model_json_schema(),\n",
    "                              options={\n",
    "                                  \"temperature\": 0\n",
    "                              }\n",
    "                              )\n",
    "\n",
    "# ic(messages)\n",
    "ic(response.model)\n",
    "ic(response.total_duration * 10e-9)  # convert from ns to s\n",
    "ic(response.message.role)\n",
    "ic(response.message.content)\n",
    "ic(response.message.thinking)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c33ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_predictions(model_name, df, max_examples=None):\n",
    "    preds = []\n",
    "    failed_ids = []\n",
    "    ids = list(df.index.astype(str))\n",
    "    if max_examples is not None:\n",
    "        ids = ids[:max_examples]\n",
    "\n",
    "    total_start = time.perf_counter()\n",
    "    per_item_times = []\n",
    "\n",
    "    for idx in tqdm(ids):\n",
    "        item = df.loc[idx].to_dict()\n",
    "        messages = create_message(item)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        try:\n",
    "            response: ChatResponse = chat(model=model_name,\n",
    "                                          messages=messages,\n",
    "                                          think=False,          # No thinking because otherwise takes ~ 6hours for the whole dev dataset per thinking model\n",
    "                                          format=Score.model_json_schema(),\n",
    "                                          options={\n",
    "                                              \"temperature\": 0\n",
    "                                          }\n",
    "                                          )\n",
    "            elapsed = time.perf_counter() - start\n",
    "\n",
    "            per_item_times.append((idx, elapsed))\n",
    "\n",
    "            content = response.message.content\n",
    "            try:\n",
    "                s = Score.model_validate_json(content)\n",
    "                pred = int(s.score)\n",
    "                if pred < 1 or pred > 5:\n",
    "                    raise ValueError(\"score out of range\")\n",
    "            except Exception:\n",
    "                # Keep id of failed element so it can be removed from evaluation\n",
    "                print(\"Invalid JSON or missing/invalid score for item id:\", idx, \"content:\", content)\n",
    "                pred = None\n",
    "                failed_ids.append(str(idx))\n",
    "\n",
    "        except Exception as e:\n",
    "            elapsed = time.perf_counter() - start\n",
    "            print(f\"Error calling model {model_name} for id {idx}: {e}\")\n",
    "            pred = None\n",
    "            failed_ids.append(str(idx))\n",
    "            per_item_times.append((idx, elapsed))\n",
    "\n",
    "        preds.append({\"id\": str(idx), \"prediction\": pred, \"time\": elapsed})\n",
    "\n",
    "    total_elapsed = time.perf_counter() - total_start\n",
    "    # attach summary timings as metadata and failed ids\n",
    "    return {\n",
    "        \"predictions\": preds,\n",
    "        \"failed_ids\": failed_ids,\n",
    "        \"total_time\": total_elapsed,\n",
    "        \"per_item_times\": per_item_times,\n",
    "        \"avg_time\": sum(t for _, t in per_item_times) / len(per_item_times) if per_item_times else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ed287",
   "metadata": {},
   "source": [
    "### Run this cell and manually change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae9cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running model: llama3.1:latest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [03:18<00:00,  2.97it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/llama3.1-latest/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/llama3.1-latest/ref.jsonl\n",
      "Timing saved to ../llm-ollama/llama3.1-latest/timing.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/llama3.1-latest/predictions.jsonl on ../llm-ollama/llama3.1-latest/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.4943482359983337\n",
      "Spearman p-Value: 1.4601503972407373e-37\n",
      "----------\n",
      "Accuracy: 0.7193877551020408 (423/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "\n",
      "=== Running model: olmo-3:7b-instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [03:30<00:00,  2.79it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/olmo-3-7b-instruct/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/olmo-3-7b-instruct/ref.jsonl\n",
      "Timing saved to ../llm-ollama/olmo-3-7b-instruct/timing.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/olmo-3-7b-instruct/predictions.jsonl on ../llm-ollama/olmo-3-7b-instruct/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.36649972972283523\n",
      "Spearman p-Value: 3.931040774206021e-20\n",
      "----------\n",
      "Accuracy: 0.5578231292517006 (328/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "\n",
      "=== Running model: granite3.3:latest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [03:19<00:00,  2.95it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/granite3.3-latest/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/granite3.3-latest/ref.jsonl\n",
      "Timing saved to ../llm-ollama/granite3.3-latest/timing.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/granite3.3-latest/predictions.jsonl on ../llm-ollama/granite3.3-latest/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.4828563815068942\n",
      "Spearman p-Value: 1.1264137444260875e-35\n",
      "----------\n",
      "Accuracy: 0.5799319727891157 (341/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "\n",
      "=== Running model: ministral-3:latest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [04:36<00:00,  2.13it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/ministral-3-latest/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/ministral-3-latest/ref.jsonl\n",
      "Timing saved to ../llm-ollama/ministral-3-latest/timing.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/ministral-3-latest/predictions.jsonl on ../llm-ollama/ministral-3-latest/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.5830923225755714\n",
      "Spearman p-Value: 7.545086025322315e-55\n",
      "----------\n",
      "Accuracy: 0.5476190476190477 (322/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "\n",
      "=== Running model: qwen3:latest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [03:59<00:00,  2.45it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/qwen3-latest/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/qwen3-latest/ref.jsonl\n",
      "Timing saved to ../llm-ollama/qwen3-latest/timing.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/qwen3-latest/predictions.jsonl on ../llm-ollama/qwen3-latest/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.5733452609839235\n",
      "Spearman p-Value: 1.0959055070644954e-52\n",
      "----------\n",
      "Accuracy: 0.6598639455782312 (388/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "\n",
      "=== Running model: deepseek-r1:latest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [04:00<00:00,  2.44it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/deepseek-r1-latest/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/deepseek-r1-latest/ref.jsonl\n",
      "Timing saved to ../llm-ollama/deepseek-r1-latest/timing.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/deepseek-r1-latest/predictions.jsonl on ../llm-ollama/deepseek-r1-latest/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.5477998321708117\n",
      "Spearman p-Value: 2.3758214682578052e-47\n",
      "----------\n",
      "Accuracy: 0.6513605442176871 (383/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "\n",
      "=== Running model: gemma3:latest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [05:29<00:00,  1.78it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../llm-ollama/gemma3-latest/predictions.jsonl\n",
      "Gold data saved to ../llm-ollama/gemma3-latest/ref.jsonl\n",
      "Timing saved to ../llm-ollama/gemma3-latest/timing.txt\n",
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/gemma3-latest/predictions.jsonl on ../llm-ollama/gemma3-latest/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.4673515563127601\n",
      "Spearman p-Value: 3.0703377733995533e-33\n",
      "----------\n",
      "Accuracy: 0.6105442176870748 (359/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file ../llm-ollama/gemma3-latest/predictions.jsonl on ../llm-ollama/gemma3-latest/ref.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.4673515563127601\n",
      "Spearman p-Value: 3.0703377733995533e-33\n",
      "----------\n",
      "Accuracy: 0.6105442176870748 (359/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Run smoke-test for each model in `model_names`\n",
    "MAX_EXAMPLES = None  # set to an int to limit samples per model\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n=== Running model: {model_name} ===\")\n",
    "    OUT_DIR = f\"../llm-ollama/zero-shot/{model_name.replace(':', '-') }\"\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # get predictions (may take a while if MAX_EXAMPLES is None)\n",
    "    res = get_dev_predictions(model_name, df_dev, max_examples=MAX_EXAMPLES)\n",
    "\n",
    "    preds = res[\"predictions\"]\n",
    "\n",
    "    pred_file = os.path.join(OUT_DIR, \"predictions.jsonl\")\n",
    "    with open(pred_file, \"w\") as f:\n",
    "        for p in preds:\n",
    "            f.write(json.dumps({\"id\": p[\"id\"], \"prediction\": p[\"prediction\"]}) + \"\\n\")\n",
    "\n",
    "    # Save failed ids so they can be excluded from scoring\n",
    "    failed_file = os.path.join(OUT_DIR, \"failed_ids.jsonl\")\n",
    "    with open(failed_file, \"w\") as f:\n",
    "        for fid in res.get(\"failed_ids\", []):\n",
    "            f.write(json.dumps({\"id\": fid}) + \"\\n\")\n",
    "\n",
    "    # Save timing info and per-item times\n",
    "    timing_file = os.path.join(OUT_DIR, \"timing.txt\")\n",
    "    with open(timing_file, \"w\") as f:\n",
    "        f.write(f\"total_time_sec: {res['total_time']:.4f}\\n\")\n",
    "        f.write(f\"avg_time_sec: {res['avg_time']:.4f}\\n\")\n",
    "        f.write(\"per_item_times_sec:\\n\")\n",
    "        for idx, t in res[\"per_item_times\"]:\n",
    "            f.write(f\"{idx}: {t:.4f}\\n\")\n",
    "\n",
    "    # Create ref.jsonl from df_dev (respect MAX_EXAMPLES) inside the model folder\n",
    "    # Exclude any ids that failed JSON parsing so they won't be evaluated\n",
    "    failed_set = set(res.get(\"failed_ids\", []))\n",
    "    ref_file = os.path.join(OUT_DIR, \"ref.jsonl\")\n",
    "    with open(ref_file, \"w\") as f:\n",
    "        for idx, row in df_dev.iterrows():\n",
    "            if MAX_EXAMPLES is not None and int(idx) >= MAX_EXAMPLES:\n",
    "                break\n",
    "            if str(idx) in failed_set:\n",
    "                # skip items that produced invalid JSON for this model\n",
    "                continue\n",
    "            f.write(json.dumps({\"id\": str(idx), \"label\": row[\"choices\"]}) + \"\\n\")\n",
    "\n",
    "    print(f\"Predictions saved to {pred_file}\")\n",
    "    print(f\"Gold data saved to {ref_file}\")\n",
    "    print(f\"Timing saved to {timing_file}\")\n",
    "\n",
    "    # Run scoring script for this model outputs\n",
    "    res = subprocess.run([\"python\", \"../score/scoring.py\", ref_file, pred_file, os.path.join(OUT_DIR, \"score.json\")], capture_output=True, text=True)\n",
    "    print(res.stdout)\n",
    "    if res.stderr:\n",
    "        print(\"Scoring STDERR:\")\n",
    "        print(res.stderr)\n",
    "\n",
    "    # \n",
    "    subprocess.run([\"ollama\", \"stop\", model_name], check=False)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32541ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
